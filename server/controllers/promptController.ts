import { RequestHandler } from 'express';
import { ServerError } from '../../types/types';
// import { fileURLToPath } from 'url';
// import fs from 'fs';
// import path from 'path';

type TopicKey = 'AI intelligence' | 'Free will';

type TopicLookupTable = {
  [K in TopicKey]: string;
};

export const parseArguments: RequestHandler = async (_req, res, next) => {
  const { userArguments, aiArguments } = res.locals;

  console.log('USER ARG: ', userArguments);
  console.log('AI ARG: ', aiArguments);

  const maxLen = Math.max(userArguments.length, aiArguments.length);

  let parsedArguments = '';

  //  parseArguments:
  //   Round 1 (AI): res.locals.ai_argument[0]
  //   Round 1 (user): res.locals.user_argument[0]
  //   Round 2 (AI): res.locals.ai_argument[1]
  //   Round 2 (user): res.locals.user_argument[1]
  //   ...

  for (let i = 0; i < maxLen; i++) {
    if (aiArguments[i]) {
      // Add to parsed Arguments
      parsedArguments += `Round ${i + 1} (AI): ${aiArguments[i]} \n\n`;
    }

    if (userArguments[i]) {
      parsedArguments += `Round ${i + 1} (User): ${userArguments[i]} \n\n`;
    }
  }

  res.locals.parsedArguments = parsedArguments;
  console.log('parsedArguments: ', parsedArguments);
  console.log('aguments parsed successfully');
  return next();
};

export const parseTopic: RequestHandler = async (_req, res, next) => {
  const { topic } = res.locals;

  const topicLookupTable: TopicLookupTable = {
    'AI intelligence': 'AI could eventually have true intelligence',
    'Free will': 'Do humans have free will?',
  };

  const roleLookupTable: TopicLookupTable = {
    'AI intelligence': 'philosopher of artificial intelligence',
    'Free will': 'professor of philosophy',
  };

  const parsedTopic =
    (topic as TopicKey) in topicLookupTable
      ? topicLookupTable[topic as TopicKey]
      : 'Unknown topic';

  const parsedRole =
    (topic as TopicKey) in roleLookupTable
      ? roleLookupTable[topic as TopicKey]
      : 'Unknown role';

  if (parsedTopic === 'Unknown topic' || parsedRole === 'Unknown role') {
    const error: ServerError = {
      log: 'unknown topic',
      status: 500,
      message: { err: 'cannot get a valid debate topic' },
    };

    return next(error);
  }
  res.locals.role = parsedRole;
  res.locals.parsedTopic = parsedTopic;
  console.log('parsedTopic: ', parsedTopic);
  console.log('topic parsed successfully');
  return next();
};

export const customizePrompts: RequestHandler = async (_req, res, next) => {
  const { parsedArguments, parsedTopic, userSide } = res.locals;
  const argumentString = parsedArguments
    ? parsedArguments
    : 'The initial argument is to be generated by AI.';

  const ai_side = userSide === 'pro' ? 'against' : 'for';

  const role = `You are a ${res.locals.role} in a back and forth dialogue with a user, where each person gets 3 rounds to make their argument total.`;
  const goal = `You are crafting a three-part argument ${ai_side} the idea that ${parsedTopic}. 
    `;
  const instructions = `
  You are currently in the round ${res.locals.round} of the debate with a human (user). You are representing the AI side.
  This is the existing debate between the user and AI so far: 
  ${argumentString}

  Please follow these instructions carefully: 
    - You will be sending one part of your argument at a time. 
    - For the first part of your argument, you will be crafting an initial argument about the topic and you will be expecting a rebuttal. 
    - After that, you will craft your next two parts of the argument by responding to particular elements of the user's rebuttal. 
    - You will be crafting each section of argument in response to the user's rebuttal of your previous claim. If user has made a claim or rebuttal, you should reponde with a direct and strong assertive counter argument. If user has a point that is particularly strong, you should acknowledge it and provide a counter argument. If user has a point that is particularly weak, you should point out the weakness, shame the user, and provide a super strong counter argument.
    - Be as conversational as possible in your argument - be personal and natural
    - You are acting as a person (a conversation partner). Do not be too formal and literal in your arguments. 
  `;

  const numOfPairs = 6;

  const responseFormat = `
  Your response will be passed to the frontend as a JSON object. Follow these instructions carefully: 
    - Do not include the word 'json' in your response. 
    - Your response should be a JSON object that contains ${numOfPairs} key-value pairs.
    - The first key should be "ai_argument" and have the value set as your counter argument.
    - The second key should be "ai_reasoning" and have the value set as your reason for providing this counter argument.
    - The third key should be "ai_strong_point" and have the value set as the perceived strengths of your counter argument.
    - The fourth key should be "ai_weak_point" and have the value set as a the perceived weaknesses of your counter argument.
    - The fifth key should be "user_strong_point" and have the value set as the perceived strengths in the user's latest argument.
    - The sixth key should be "user_weak_point" and have the value set as a the perceived weaknesses in the user's latest argument.
  `;
  // (if a user argument exists in the ${argumentString})
  const systemContent = role + goal + instructions + responseFormat;

  res.locals.systemContent = systemContent;
  console.log('systemContent: ', systemContent);
  console.log('prompts customized successfully');
  return next();
};

export const parseDebateHistoryFeedback: RequestHandler = async (
  _req,
  res,
  next
) => {
  const {
    userArguments,
    aiArguments,
    aiReasonings,
    aiStrongPoints,
    aiWeakPoints,
    userStrongPoints,
    userWeakPoints,
  } = res.locals;

  console.log('AI Weakpoints', aiWeakPoints);
  //  parsed history and feedback:
  //   Round 1 (AI's argument): res.locals.ai_argument[0]
  //   Round 1 (weak points in AI's argument): res.locals.ai_weak_point[0]
  //   Round 1 (strong points in AI's argument): res.locals.ai_strong_point[0]
  //   Round 1 (user's argument): res.locals.user_argument[0]
  //   Round 1 (weak points in user's argument): res.locals.user_weak_point[0]
  //   Round 1 (strong points in user's argument): res.locals.user_strong_point[0]
  //   Round 2 (AI's argument): res.locals.ai_argument[1]
  //   Round 2 (weak points in AI's argument): res.locals.ai_weak_point[1]
  //   Round 2 (strong points in AI's argument): res.locals.ai_strong_point[1]
  //   Round 2 (user's argument): res.locals.user_argument[1]
  //   Round 2 (weak points in user's argument): res.locals.user_weak_point[1]
  //   Round 2 (strong points in user's argument): res.locals.user_strong_point[1]
  //   Round 3 (AI's argument): res.locals.ai_argument[2]
  //   Round 3 (weak points in AI's argument): res.locals.ai_weak_point[2]
  //   Round 3 (strong points in AI's argument): res.locals.ai_strong_point[2]
  //   Round 3 (user's argument): res.locals.user_argument[2]
  //   Round 3 (weak points in user's argument): res.locals.user_weak_point[2]
  //   Round 3 (strong points in user's argument): res.locals.user_strong_point[2]
  //

  // FIRST AI RESPONSE:[ ai response 1] +[ ai weak points 1] + [ai strong points 1]
  // FIRST USER RESPONSE: [user response 1] + [ ai response 1] +[ ai weak points 1] + [ai strong points 1]
  // SECOND AI RESPONSE: [user response 1] + [ ai response 1, aiReponse 2] +[ ai weak points 1, 2] + [ai strong points 1, 3], [user weak points 1] + [user strong points 2]
  // SECOND USER RESPONSE: [user response 1, 2] + [ ai response 1, aiReponse 2] +[ ai weak points 1, 2] + [ai strong points 1, 3], [user weak points 1] + [user strong points 2]
  // THIRD AI RESPONSE: [user response 1, 2] + [ ai response 1, aiReponse 2, 3] +[ ai weak points 1, 2, 3] + [ai strong points 1, 2, 3], [user weak points 1, 2] + [user strong points 1, 2]
  // THIRD USER RESPONSE: [user response 1, 2, 3] + [ ai response 1, aiReponse 2] +[ ai weak points 1, 2] + [ai strong points 1, 3], [user weak points 1] + [user strong points 2]
  // FOURTH AI RESPONSE: [user response 1, 2, 3] + [ ai response 1, aiReponse 2, 3, 4] +[ ai weak points 1, 2, 3, 4] + [ai strong points 1, 2, 3, 4], [user weak points 1, 2, 3] + [user strong points 1, 2, 3]

  const maxLen = Math.max(userArguments.length, aiArguments.length);

  let debateHistoryFeedback = '';

  for (let i = 0; i < maxLen - 1; i++) {
    if (aiArguments[i]) {
      console.log(debateHistoryFeedback);
      // Add to parsed Arguments
      debateHistoryFeedback += `Round ${i + 1} (AI's argument): ${aiArguments[i]} \n`;
      debateHistoryFeedback += `Round ${i + 1} (weak points in AI's argument): ${aiWeakPoints[i]} \n`;
      debateHistoryFeedback += `Round ${i + 1} (strong points in AI's argument): ${aiStrongPoints[i]} \n`;
    }

    if (userArguments[i]) {
      debateHistoryFeedback += `Round ${i + 1} (User's Argument): ${userArguments[i]} \n`;
      debateHistoryFeedback += `Round ${i + 1} (weak points in user's argument): ${userWeakPoints[i + 1]} \n`;
      debateHistoryFeedback += `Round ${i + 1} (weak strong in user's argument): ${userStrongPoints[i + 1]} \n`;
    }
  }

  res.locals.debateHistoryFeedback = debateHistoryFeedback;
  console.log('debateHistoryFeedback: ', debateHistoryFeedback);
  console.log('debate history parsed successfully');
  return next();
};

export const customizeEvaluationPrompts: RequestHandler = async (
  _req,
  res,
  next
) => {
  const { parsedTopic, userSide } = res.locals;
  const ai_side = userSide === 'pro' ? 'against' : 'for';

  const judgementRole = `You will act as the judge in a debate between a human (user) and an AI. The topic of the debate is: ${parsedTopic}. 
  The ai side is ${ai_side} the topic, and the user side is on another side.
  `;

  const judgementGoal = `After reviewing the entire debate, provide a comprehensive analysis`;

  const rubrics = `
Assessment Rubrics:

Please follow the assessment rubrics below:

1. Comprehensive Assessment (comp_assessment)

Criteria for Assessment:

Clarity and Coherence: Evaluate how clearly and logically the user presented their arguments. Were the points easy to understand?
Use of Evidence: Consider the strength and relevance of evidence provided to support claims. Did the user reference credible sources or data?
Counterarguments: Assess the ability to address and refute opposing arguments. Did the user effectively counter the AI's points?
Persuasiveness: Reflect on how convincingly the user articulated their position. Did the user engage the audience?
Structure: Review the overall organization of arguments. Was there a logical flow to the points made?
Output: Provide a narrative that highlights both strengths and weaknesses in the user’s performance based on the criteria above.

2. Determining the Winner (winner)

Criteria for Winner Determination:

Overall Effectiveness: Compare the impact of both the user’s and AI’s arguments. Which side presented a more compelling case?
Balance of Arguments: Evaluate which side had stronger points overall, taking into account the quality of evidence and effectiveness of rebuttals.
Audience Engagement: Consider which side resonated better with the audience (or a hypothetical audience).
Output: Choose either "user" or "ai" based on the overall effectiveness of their arguments.

3. Constructive Advice for Improvement (ai_advice)

Criteria for Advice:

Identified Weaknesses: Based on the performance analysis, highlight specific areas where the user struggled (e.g., lack of evidence, weak rebuttals).
Skills Development: Suggest ways to enhance skills such as research, structuring arguments, or public speaking.
Future Strategies: Offer practical tips for future debates, such as practicing counterarguments or improving clarity.
Output: Provide a constructive feedback statement focused on improvement.

4. Scoring Evaluation (user_score and ai_score)

Scoring Criteria:

Scale Definition: Use a scale from 1 to 10, where:
1-3: Poor performance, significant improvement needed.
4-6: Average performance, with identifiable weaknesses and some strengths.
7-9: Strong performance, with clear strengths and minor weaknesses.
10: Exceptional performance, no significant weaknesses observed.
Output: Assign a score based on the overall assessment of each side's debate performance, considering the clarity, evidence, rebuttals, and persuasiveness.

5. Identifying User Blind Spots (user_blind_spots)

Criteria for Identification:

Recurrent Issues: Look for patterns in the user’s arguments where they consistently lacked depth or clarity.
Missed Opportunities: Identify any points where the user could have provided stronger arguments or countered the AI more effectively.
Growth Potential: Highlight areas where the user has room for growth, whether in argumentation, engagement, or knowledge of the topic.
Output: Provide a concise statement outlining the user’s blind spots to encourage focused improvement in future debates.
`;

  const contextFormat = `
Context:

In the prompt, I will provide the debate chat history, detailing each round with the user's argument, the AI's argument, and a third-party analysis of each side's strong and weak points.
The format of the debate history and third party analysis for each round should be in the following format:

Round 1:
User Argument: [user's argument for this round]
AI Argument: [AI's argument for this round]
User Strong Points: [user's strong points in the last argument]
User Weak Points: [user's weak points in the last argument]
AI Strong Points: [AI's strong points in the current argument]
AI Weak Points: [AI's weak points in the current argument]

Round 2:
User Argument: [user's argument for this round]
AI Argument: [AI's argument for this round]
User Strong Points: [user's strong points in the last argument]
User Weak Points: [user's weak points in the last argument]
AI Strong Points: [AI's strong points in the current argument]
AI Weak Points: [AI's weak points in the current argument]
(Continue this format for additional rounds as needed.)
`;

  const outputFormat = `
Output Format: 

Output your response in a JSON object with the following fields, do not include the word "json" in it:

"comp_assessment": A string that offers a detailed assessment of the user’s debate performance, highlighting strengths and weaknesses.
"winner": Indicate whether the "user" or "ai" performed better in the debate.
"ai_advice": A string offering constructive advice for the user to improve their debate skills.
"user_score": A numerical score (1-10) evaluating the user’s debate performance.
"ai_score": A numerical score (1-10) evaluating the AI’s debate performance.
"user_blind_spots": A string identifying any blind spots or areas for improvement that the user could address in future debates.
`;

  const systemEvaluationContent =
    judgementRole + judgementGoal + contextFormat + rubrics + outputFormat;

  res.locals.systemEvaluationContent = systemEvaluationContent;

  return next();
};
